name: Notebook Deployment - Databricks
on:
  workflow_call:
    inputs:
      databricks_code_path:
        description: Databricks Service Path
        type: string
        required: true
      
      service_code_file:
        description: python code file path
        type: string
        required: true
    
  #secrets:
    #DATABRICKS_HOST_URL:
      #description:   'Databricks workspace URL'
      #required: true
    #DATABRICKS_TOKEN:
      #description: 'Access token for Databricks CLI'
      #required: true

jobs:
  push_to_db:
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout current repository
        uses: actions/checkout@v3.3.0
      
      - name: Set up python
        uses: actions/setup-python@v4
        with:
          python-version: 3.9

      - name: Databricks CLI config
        run: |
            pip install databricks-cli 
            cat > ~/.databrickscfg << EOF 
            [DEFAULT] 
            host = ${{ secrets.DATABRICKS_HOST_URL }} 
            token = ${{ secrets.DATABRICKS_TOKEN }} 
            jobs-api-version = 2.1 
            EOF
      - name: Trigger notebook in prod
        uses: databricks/run-notebook@v0
        with:
          databricks-host: ${{ secrets.DATABRICKS_HOST_URL }} 
          databricks-token: ${{ secrets.DATABRICKS_TOKEN }} 
          local-notebook-path: ${{ inputs.service_code_file }}
          # The cluster JSON below is for AWS workspaces. On Azure and GCP, set
          # node_type_id to an appropriate node type, e.g. "Standard_D3_v2" for
          # Azure or "n1-highmem-4" for GCP
          new-cluster-json: >
            {
              "num_workers": 1,
              "spark_version": "11.3.x-scala2.12",
              "node_type_id": "i3.xlarge"
            }
          # Grant users in the "devops" group view permission on the
          # notebook results
          access-control-list-json: >
            [
              {
                "group_name": "devops",
                "permission_level": "CAN_VIEW"
              }
            ]
      - name: Deploy code to databricks workspace
        run: |
            databricks workspace import ${{ inputs.service_code_file }} ${{ inputs.databricks_code_path }} --language python --overwrite
