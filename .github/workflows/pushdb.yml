name: Notebook Deployment - Databricks
on:
  workflow_call:
    inputs:
      databricks_code_path:
        description: Databricks Service Path
        type: string
        required: true
      
      service_code_file:
        description: python code file path
        type: string
        required: true
env:
  DATABRICKS_HOST: https://8350567918231961.1.gcp.databricks.com
  DATABRICKS_TOKEN: dapi4749708e6e8716e81ac42f41a38687ae
    
  #secrets:
    #DATABRICKS_HOST_URL:
      #description:   'Databricks workspace URL'
      #required: true
    #DATABRICKS_TOKEN:
      #description: 'Access token for Databricks CLI'
      #required: true

jobs:
  run_temp_nb:
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout current repository
        uses: actions/checkout@v3.3.0
      
      - name: Set up python
        uses: actions/setup-python@v4
        with:
          python-version: 3.9
          
      - name: Trigger notebook in staging
        uses: databricks/run-notebook@v0.0.3
        with:
          databricks-host: ${{ secrets.DATABRICKS_HOST_URL }}
          databricks-token: ${{ secrets.DATABRICKS_TOKEN }}
          local-notebook-path: ${{ inputs.service_code_file }}
          # The cluster JSON below is for AWS workspaces. On Azure and GCP, set
          # node_type_id to an appropriate node type, e.g. "Standard_D3_v2" for
          # Azure or "n1-highmem-4" for GCP
          #  git-commit: ${{ github.event.head.sha }}
          existing-cluster-id: 0621-225508-8s781rm9
          #new-cluster-json: >
           # {
             # "num_workers": 1,
             # "spark_version": "11.3.x-scala2.12",
            #  "node_type_id": "n1-highmem-4"
           # }
          # Grant users in the "devops" group view permission on the
          # notebook results
          
  push_to_dbs:
      runs-on: ubuntu-latest
    
      steps:
      - name: Checkout current repository
        uses: actions/checkout@v3.3.0
      
      - name: Set up python
        uses: actions/setup-python@v4
        with:
          python-version: 3.9
        
      - name: Databricks CLI config
        run: |
            pip install databricks-cli 
            cat > ~/.databrickscfg << EOF 
            [DEFAULT] 
            host = ${{ secrets.DATABRICKS_HOST_URL }} 
            token = ${{ secrets.DATABRICKS_TOKEN }} 
            jobs-api-version = 2.1 
            EOF 
    
      - name: Import code file to databricks workspace
        run: |
            databricks workspace import src/code/py.py ${{ inputs.databricks_code_path }} --language python --overwrite
     
      - name: Import config file to databricks workspace
        run: |
            databricks workspace import_dir etl/config /Users/user1/etl/config
     
      - name: Copy the libraries from github repo to DBFS
        run: |
            databricks fs cp etl-2.1-assembly.jar dbfs:/user1/etl/etl-2.1-assembly.jar
     
